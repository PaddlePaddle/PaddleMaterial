Global:
  name: 'm2m-model'      # Warning: 'debug' and 'test' are reserved name that have a special behavior
  gpus: 1                     # Multi-gpu is not implemented on this branch
Trainer:
  epochs: 500
  seed: 42
  start_eval_epoch: 1
  eval_freq: 1 # set 0 to disable evaluation during training
  clip_grad: null
  ema_decay: 0      # 'Amount of EMA decay, 0 means off. A reasonable value  is 0.999.'
  pretrained_model_path: null # set your pretrained model path here
  test_only: null
  check_val_every_n_epochs: 1
  sample_every_val: 1
  val_check_interval: null
  samples_to_generate: 16       # We advise to set it to 2 x batch_size maximum
  samples_to_save: 1
  chains_to_save: 1
  # log_every_steps: 500
  number_chain_steps: 1        # Number of frames in each gif
  final_model_samples_to_generate: 512
  final_model_samples_to_save: 512
  final_model_chains_to_save: 512
  evaluate_all_checkpoints: False
  Optimizer:
    __name__: AdamW
    beta1: 0.9
    beta2: 0.999
    lr:
      __name__: ReduceOnPlateau
      learning_rate: 0.002
      factor: 0.6
      by_epoch: True
      patience: 30
      min_lr: 0.0001
      indicator: "train_loss"
Tracker:
  log:
    log_freq: 500 # log frequency [step]
  save:
    output_dir: ./output/DeNMR_CHnmr/MultiModalDecoder
    save_freq: 100 # set 0 to disable saving during training
    is_save_traj: False
Metric:
  __name__: CSPMetric
  gt_file_path: "./data/mp_20/test.csv"
Model:
  __name__: MultiModalDecoder
  nmr_encoder:
    __name__: Transformer
    pretrained_path: "./output/DeNMR_CHnmr/CLIP/checkpoints/epoch=35.pdparams"
    enc_voc_size: 5450
    max_len: 256
    d_model: 256
    ffn_hidden: 1024
    n_head: 8
    n_layers: 3
    drop_prob: 0.0
    projector:
      __name__: Linear
      pretrained_path: "./output/DeNMR_CHnmr/CLIP/checkpoints/epoch=35.pdparams"
      outfeatures: 512
  graph_decoder:
    __name__: DiffGraphFormer
    pretrained_path: "./output/DeNMR_CHnmr/DiffGraphFormer/checkpoints/epoch=438.pdparams"
    hidden_mlp_dims: {
      'X': 256,
      'E': 128,
      'y': 256
    }
    hidden_dims: {
      'dx': 256,
      'de': 64,
      'dy': 256,
      'n_head': 8,
      'dim_ffX': 256,
      'dim_ffE': 128,
      'dim_ffy': 256
    }
    num_layers: 5
    diffusion_model:
      type: 'discrete_condition'
      transition: 'marginal' # uniform or marginal
      model: 'graph_tf'
      diffusion_steps: 120
      diffusion_noise_schedule: 'cosine' # 'cosine', 'polynomial_2'
      extra_features: 'all' # 'all', 'cycles', 'eigenvalues' or null
      # Do not set hidden_mlp_E, dim_ffE too high, computing large tensors on the edges is costly
      # At the moment (03/08), y contains quite little information
      lambda_train: [5, 0]
      conditdim: 512
      number_chain_steps: 1
    vocab_dim: 256
  clip:
    __name__: ContrastiveModel
    graph_encoder:
      pretrained_model_path: "./output/DeNMR_CHnmr/DiffGraphFormer/checkpoints/epoch=438.pdparams"
      n_layers_GT: 5
      input_dims:
        X: 17
        E: 5
        y: 525
      hidden_mlp_dims:
        X: 256
        E: 128
        y: 256
      hidden_dims: {'dx': 256, 'de': 64, 'dy': 256, 'n_head': 8, 'dim_ffX': 256, 'dim_ffE': 128, 'dim_ffy': 256}
      output_dims: {'X': 9, 'E': 5, 'y': 0}
      vocab_dim: 256
    nmr_encoder:
      enc_voc_size: 5450
      max_len: 256
      d_model: 256
      ffn_hidden: 1024
      n_head: 8
      n_layers: 3
      drop_prob: 0.0
      projector:
        __name__: Linear
        #pretrained_path: "./output/DeNMR_CHnmr/CLIP/checkpoints/epoch=35.pdparams"
        outfeatures: 512
    diffusion_model: # to be deleted
      extra_features: 'all' # 'all', 'cycles', 'eigenvalues' or null
      conditdim: 512
  connector: # null if no connector
    __name__: "DiffPrior"
    prior_network:
      dim: 512
      num_timesteps: 1000
      num_time_embeds: 1
      num_graph_embeds: 1
      num_text_embeds: 1
      max_text_len: 256
      self_cond: False
      depth: 6
      dim_head: 64
      heads: 8
    pretrained_path: "./output/DeNMR_CHnmr/DiffPrior/checkpoints/latest.pdparams"
    graph_embed_dim: 512
    timesteps: 1000
    sample_timesteps: 64
    cond_drop_prob: 0.1
    text_cond_drop_prob: 0.05
    graph_cond_drop_prob: 0.05
    loss_type: "l2"
    predict_x_start: true
    predict_v: false
    beta_schedule: "cosine"
    condition_on_text_encodings: false
    sampling_clamp_l2norm: false
    sampling_final_clamp_l2norm: false
    training_clamp_l2norm: false
    init_graph_embed_l2norm: false
    graph_embed_scale: null
  model_dtype: 'float32'
  diffusion_model:
    extra_features: 'all'
    conditdim: 512
Dataset:
  train:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/train.csv"
      vocab_path: "./data/CHnmr/vocab.src"
      datadir: "./data/CHnmr"
      cache: True
      remove_h: True
    loader:
      num_workers: 0
      use_shared_memory: False
    sampler:
      __name__: DistributedBatchSampler
      shuffle: True
      drop_last: False
      batch_size: 32
  val:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/val.csv"
      vocab_path: "./data/CHnmr/vocab.src"
      cache: True
      remove_h: True
    sampler:
      __name__: BatchSampler
      shuffle: True
      drop_last: False
      batch_size: 32
  test:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/test.csv"
      vocab_path: "./data/CHnmr/vocab.src"
      cache: True
      remove_h: True
    sampler:
      __name__: BatchSampler
      shuffle: True
      drop_last: False
      batch_size: 32
