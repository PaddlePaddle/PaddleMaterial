Trainer:
  epochs: 2000
  seed: 100
  start_eval_epoch: 1
  eval_freq: 1 # set 0 to disable evaluation during training
  clip_grad: null
  ema_decay: 0      # 'Amount of EMA decay, 0 means off. A reasonable value  is 0.999.'
  pretrained_model_path: "./pretrained/step4_noprior_best_v1.pdparams" # "./output/DeNMR_CHnmr/MultiModalDecoder_noPrior/v5/checkpoints/epoch_1700.pdparams" #"./output/step4_noprior_best_v1.pdparams" #null # set your pretrained model path here
  Optimizer:
    __name__: AdamW
    beta1: 0.9
    beta2: 0.999
    lr: 0.001
    weight_decay: 1e-12
    epsilon: 1e-8
    amsgrad: True
  vocab_dim: 256
Sampler:
  sample_every_val: 2000
  visual_num: 10 # number of visualize molecules in the sample
  chains_to_save: 5 # less sample batch size, select representative sample in sample batch for visualize
  number_chain_steps: 10 # Number of frames in each gif
  sample_batch_iters: 1 # Number of sample batches
Tracker:
  log:
    log_freq: 1 # log frequency [step]
    out_dict: # none is default, it output all loss and metrics, empty dict means no output
      loss:
        train: {"train_loss",}
        eval: {"train_loss", "nll"}
      metric:
        train: {}
        eval: {"val_nll"}
        sample: {"Accuracy",}
    flag_train_step: True # decidet output training batch iteration process or not
    flag_eval_step: False
    flag_sample_metric: True # decidet output tanimot similarity calculationg process or not
  save:
    output_dir: ./output/DeNMR_CHnmr/MultiModalDecoder_noPrior/test_v4/
    save_freq: 100 # set 0 to disable saving during training
    is_save_traj: False
Model:
  __name__: MultiModalDecoder
  nmr_encoder:
    pretrained_path: "./pretrained/step2_best.pdparams" #"/home/liuxuwei01/PaddleMaterial/output/step2_init_weight.pdparams" #"./output/step2_best.pdparams"
    dim_enc_H: 1024
    dimff_enc_H: 2048
    dim_enc_C: 256
    dimff_enc_C: 512
    ffn_hidden: 512
    n_head: 8
    n_layers: 3
    drop_prob: 0.0 # 0.0
    seq_len_H1: 20
    seq_len_C13: 75
    peakwidthemb_num: 70
    integralemb_num: 26
  graph_decoder:
    __name__: DiffGraphFormer
    pretrained_path: "./pretrained/step1_best.pdparams" #"./output/step1_best_v1.pdparams"  #"./output/DeNMR_CHnmr/DiffGraphFormer/checkpoints/epoch_200.pdparams"
    hidden_mlp_dims: {
      'X': 256,
      'E': 128,
      'y': 256
    }
    hidden_dims: {
      'dx': 256,
      'de': 64,
      'dy': 256,
      'n_head': 8,
      'dim_ffX': 256,
      'dim_ffE': 128,
      'dim_ffy': 256
    }
    num_layers: 5
    vocab_dim: 256
    diffusion_model:
      type: 'discrete_condition'
      transition: 'marginal' # uniform or marginal
      model: 'graph_tf'
      diffusion_steps: 500
      diffusion_noise_schedule: 'cosine' # 'cosine', 'polynomial_2'
      extra_features: 'all' # 'all', 'cycles', 'eigenvalues' or null
      # Do not set hidden_mlp_E, dim_ffE too high, computing large tensors on the edges is costly
      # At the moment (03/08), y contains quite little information
      lambda_train: [5, 0]
      conditdim: 512
      number_chain_steps: 1
  connector: null
  model_dtype: 'float32'
  diffusion_model:
    extra_features: 'all'
    conditdim: 512
Dataset:
  train:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/train.csv"
      vocab_peakwidth_path: "./molecule_generation/H1_statistic/delta_distribution.csv"
      vocab_split_path: "./molecule_generation/H1_statistic/split_type_distribution.csv"
      datadir: "./data/CHnmr"
      cache: True
      remove_h: True
      data_flag: "small" # "large"
      seq_len_H1: 20
      seq_len_C13: 75
    loader:
      num_workers: 0
      use_shared_memory: False
    sampler:
      __name__: DistributedBatchSampler
      shuffle: True
      drop_last: False
      batch_size: 512
  val:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/val.csv"
      vocab_peakwidth_path: "./molecule_generation/H1_statistic/delta_distribution.csv"
      vocab_split_path: "./molecule_generation/H1_statistic/split_type_distribution.csv"
      cache: True
      remove_h: True
      data_flag: "small" # "large"
      seq_len_H1: 20
      seq_len_C13: 75
    sampler:
      __name__: BatchSampler
      shuffle: False
      drop_last: False
      batch_size: 512
  sample:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/val.csv"
      vocab_peakwidth_path: "./molecule_generation/H1_statistic/delta_distribution.csv"
      vocab_split_path: "./molecule_generation/H1_statistic/split_type_distribution.csv"
      cache: True
      remove_h: True
      data_flag: "small" # "large"
      seq_len_H1: 20
      seq_len_C13: 75
    sampler:
      __name__: BatchSampler
      shuffle: False
      drop_last: False
      batch_size: 512
  test:
    dataset:
      __name__: CHnmrDataset
      path: "./data/CHnmr/test.csv"
      vocab_peakwidth_path: "./molecule_generation/H1_statistic/delta_distribution.csv"
      vocab_split_path: "./molecule_generation/H1_statistic/split_type_distribution.csv"
      cache: True
      remove_h: True
      data_flag: "small" # "large"
      seq_len_H1: 20
      seq_len_C13: 75
    sampler:
      __name__: BatchSampler
      shuffle: False
      drop_last: False
      batch_size: 512
